{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "k2xX1DPT21ZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cacc39-9903-412f-addd-7b20815cd2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optimum-intel as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nncf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "%pip install -Uq pip\n",
        "%pip uninstall -q -y optimum optimum-intel\n",
        "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
        "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
        "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
        "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
        "\"torch>=2.1\"\\\n",
        "\"datasets\" \\\n",
        "\"accelerate\"\\\n",
        "\"gradio>=4.19\"\\\n",
        "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.40\" \"bitsandbytes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "c81ea07b356b4dc0930d09e0925107d8",
            "48a405d762ee4cec97b265412e3194ff",
            "44a757b7d2d0448eb4d7693ac85033e3",
            "f818f346dce34eb293e2e1d02a0620b4",
            "d41a3078b9144b5cbb794de44a2e0f61",
            "b92ebfa18135471b980c2a13a8e867f6",
            "287befd8c3ef4a298c8cc06d8d902af2",
            "fe15f4c98065468f970aaf5e4b4a6132",
            "5a6a339928e6457c8967f2a6ef214397",
            "271b3bf299ae456990657ca017910957",
            "d9022c5909bd448f8e35bd80fd8fd30a",
            "11e38469ff5641ae8286b7b91de5cbe1",
            "0c0f46f668964aab98756bca8dcb0aaf",
            "2a3952c416934611b64dc6c96299d437",
            "214fb6f006ff4ee0b17a3eaa4ebde83d",
            "841c385620f34883bbeb7f78881f97d6",
            "31168702fea441bb8edca14d69a908a7",
            "1c9d10b9cf0f421b86fd118570a816e6",
            "b2bf16ee83fd47ffb4cd536bbdb13efb",
            "cb0d50e939904c61a6ae4c1e9a187b19",
            "4f6467111f6449229609c3a615c15a5a",
            "2376af59aa2a411c830757f1f7fcee72",
            "91189da9874c48e1a916dadbb499bee3",
            "629c5e879a024e62afe47b3ae3ff0b89",
            "89f848c0f9d443c49183ab19ef6ba031",
            "69536d5f4d674205a00c54c0b6af6051",
            "dc0fba71c22d4a8989829eec0d311f0e",
            "03060c54251b4dc28796c678fbd39d10",
            "a313ea293d99499dbebee43bc60d055e",
            "18dee6ba9b0840b988dc7ca65b0ae453",
            "4859cb9e1ab7408ea2523f71edc531bf",
            "7e3847f3f1714a918cd2b5c244f88c60"
          ]
        },
        "id": "Ic5Uc2Eb24Db",
        "outputId": "36ab8e78-6b49-4c5b-97bb-9aa0b571181a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c81ea07b356b4dc0930d09e0925107d8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login, whoami\n",
        "try:\n",
        "    whoami()\n",
        "    print('Authorization token already provided')\n",
        "except OSError:\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yjPrfyQt30_F"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q6hhBx7o3_hq"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tobmp9Gw3_87"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "                    \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                    \"remote_code\": False,\n",
        "                    \"start_message\": f\"<|system|>\\n{DEFAULT_SYSTEM_PROMPT}</s>\\n\",\n",
        "                    \"history_template\": \"<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}</s> \\n\",\n",
        "                    \"current_message_template\": \"<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}\"\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T9FvG-qN4B3l"
      },
      "outputs": [],
      "source": [
        "def llama_partial_text_processor(partial_text, new_text):\n",
        "    new_text = new_text.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\")\n",
        "    partial_text += new_text\n",
        "    return partial_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xC6metSx4E5L"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pt_model_id = model_config[\"model_id\"]\n",
        "pt_model_name = pt_model_id.split(\"/\")[1]\n",
        "int4_model_dir = Path(pt_model_name) / \"INT4_compressed_weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4ZJ0C3z34HqZ"
      },
      "outputs": [],
      "source": [
        "def convert_to_int4():\n",
        "    model_compression_params = {\n",
        "        \"sym\": False,\n",
        "        \"group_size\": 128,\n",
        "        \"ratio\": 0.8,\n",
        "    }\n",
        "\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_config.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
        "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
        "    if model_compression_params[\"sym\"]:\n",
        "        int4_compression_args += \" --sym\"\n",
        "    export_command_base += int4_compression_args\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
        "    ! $export_command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9W0eCVh5KZD",
        "outputId": "865b2278-4e1a-41dd-bf70-c2fd9a4f3d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-15 11:47:48.246061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-15 11:47:48.246121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-15 11:47:48.371907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-15 11:47:48.608546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 11:47:51.007356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "config.json: 100% 608/608 [00:00<00:00, 3.53MB/s]\n",
            "Framework not specified. Using pt to export the model.\n",
            "model.safetensors: 100% 2.20G/2.20G [00:23<00:00, 92.2MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 711kB/s]\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 5.75MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 263MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 1.85MB/s]\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 2.81MB/s]\n",
            "Using framework PyTorch: 2.3.0+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/optimum/exporters/openvino/model_patcher.py:467: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n",
            "/usr/local/lib/python3.10/dist-packages/optimum/exporters/openvino/model_patcher.py:503: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if _seq_len > self.embed_positions.shape[0]:\n",
            "['input_ids', 'attention_mask', 'position_ids', 'past_key_values']\n",
            "\u001b[2KMixed-Precision assignment \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m154/154\u001b[0m • \u001b[36m0:00:15\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO:nncf:Statistics of the bitwidth distribution:\n",
            "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
            "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
            "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
            "│              8 │ 30% (43 / 157)              │ 20% (40 / 154)                         │\n",
            "├────────────────┼─────────────────────────────┼────────────────────────────────────────┤\n",
            "│              4 │ 70% (114 / 157)             │ 80% (114 / 154)                        │\n",
            "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
            "\u001b[2KApplying Weight Compression \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m157/157\u001b[0m • \u001b[36m0:00:45\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "convert_to_int4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGPbCeJA5Nhe",
        "outputId": "8bb895b4-2940-4e9d-bd4b-c87eec1c7695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT4 compressed weights is 696.44 MB\n"
          ]
        }
      ],
      "source": [
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if int4_weights.exists():\n",
        "    print(f\"Size of model with INT4 compressed weights is {int4_weights.stat().st_size / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "klga4bKQ6VBi"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "\n",
        "core = ov.Core()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Ma-W4h_U2Z",
        "outputId": "b53c2c36-0578-484c-f02c-c221393bb549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from optimum.intel.openvino import OVModelForCausalLM\n",
        "\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "model_name = model_config[\"model_id\"]\n",
        "tok = AutoTokenizer.from_pretrained(int4_model_dir, trust_remote_code=True)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    int4_model_dir,\n",
        "    device = \"CPU\",\n",
        "    ov_config=ov_config,\n",
        "    config=AutoConfig.from_pretrained(int4_model_dir, trust_remote_code=True),\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "xMxKsfmuA4G-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from threading import Event, Thread\n",
        "from uuid import uuid4\n",
        "from typing import List, Tuple\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "\n",
        "model_name = model_config[\"model_id\"]\n",
        "start_message = model_config[\"start_message\"]\n",
        "history_template = model_config.get(\"history_template\")\n",
        "current_message_template = model_config.get(\"current_message_template\")\n",
        "\n",
        "examples = [\n",
        "    [\"What's your favorite way to spend a weekend?\"],\n",
        "    [\"What inspired you to pursue your current career?\"],\n",
        "    [\"How does machine learning differ from traditional programming?\"],\n",
        "    [\"What is the main conflict in the series Arcane?\"],\n",
        "    [\"What are some best practices for debugging code?\"],\n",
        "    [\"Write a poem about the changing seasons.\"],\n",
        "    [\"What are some effective strategies for expanding your vocabulary?\"]\n",
        "]\n",
        "\n",
        "max_new_tokens = 256\n",
        "\n",
        "\n",
        "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
        "    \"\"\"\n",
        "    function for conversion history stored as list pairs of user and assistant messages to tokens according to model expected conversation template\n",
        "    Params:\n",
        "      history: dialogue history\n",
        "    Returns:\n",
        "      history in token format\n",
        "    \"\"\"\n",
        "    text = start_message + \"\".join(\n",
        "        [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
        "    )\n",
        "    text += \"\".join(\n",
        "        [\n",
        "            \"\".join(\n",
        "                [\n",
        "                    current_message_template.format(\n",
        "                        num=len(history) + 1,\n",
        "                        user=history[-1][0],\n",
        "                        assistant=history[-1][1],\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    input_token = tok(text, return_tensors=\"pt\").input_ids\n",
        "    return input_token\n",
        "\n",
        "\n",
        "def user(message, history):\n",
        "    \"\"\"\n",
        "    callback function for updating user messages in interface on submit button click\n",
        "\n",
        "    Params:\n",
        "      message: current message\n",
        "      history: conversation history\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    # Append the user's message to the conversation history\n",
        "    return \"\", history + [[message, \"\"]]\n",
        "\n",
        "\n",
        "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
        "    \"\"\"\n",
        "    callback function for running chatbot on submit button click\n",
        "\n",
        "    Params:\n",
        "      history: conversation history\n",
        "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
        "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
        "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
        "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
        "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
        "      conversation_id: unique conversation identifier.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
        "    # Tokenize the messages string\n",
        "    input_ids = convert_history_to_token(history)\n",
        "    if input_ids.shape[1] > 2000:\n",
        "        history = [history[-1]]\n",
        "        input_ids = convert_history_to_token(history)\n",
        "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "\n",
        "    stream_complete = Event()\n",
        "\n",
        "    def generate_and_signal_complete():\n",
        "        \"\"\"\n",
        "        genration function for single thread\n",
        "        \"\"\"\n",
        "        global start_time\n",
        "        ov_model.generate(**generate_kwargs)\n",
        "        stream_complete.set()\n",
        "\n",
        "    t1 = Thread(target=generate_and_signal_complete)\n",
        "    t1.start()\n",
        "\n",
        "    # Initialize an empty string to store the generated text\n",
        "    partial_text = \"\"\n",
        "    for new_text in streamer:\n",
        "        partial_text = llama_partial_text_processor(partial_text, new_text)\n",
        "        history[-1][1] = partial_text\n",
        "        yield history\n",
        "\n",
        "\n",
        "def request_cancel():\n",
        "    ov_model.request.cancel()\n",
        "\n",
        "\n",
        "def get_uuid():\n",
        "    \"\"\"\n",
        "    universal unique identifier for thread\n",
        "    \"\"\"\n",
        "    return str(uuid4())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KBDBLIsatB88",
        "outputId": "f05f5431-ea35-497a-e0e4-968a97069010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-12 (_do_normal_analytics_request):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n",
            "    response = connection.handle_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\", line 99, in handle_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\", line 76, in handle_request\n",
            "    stream = self._connect(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\", line 122, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\", line 205, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectTimeout: timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/analytics.py\", line 70, in _do_normal_analytics_request\n",
            "    data[\"ip_address\"] = get_local_ip_address()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/analytics.py\", line 131, in get_local_ip_address\n",
            "    ip_address = httpx.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_api.py\", line 198, in get\n",
            "    return request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_api.py\", line 106, in request\n",
            "    return client.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 827, in request\n",
            "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 232, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectTimeout: timed out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on public URL: https://345847c5ebcaedd65e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://345847c5ebcaedd65e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
        ") as demo:\n",
        "    conversation_id = gr.State(get_uuid)\n",
        "    gr.Markdown(f\"\"\"<h1><center>OpenVINO {model_name} Chatbot</center></h1>\"\"\")\n",
        "    chatbot = gr.Chatbot(height=500)\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            msg = gr.Textbox(\n",
        "                label=\"Chat Message Box\",\n",
        "                placeholder=\"Chat Message Box\",\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "            )\n",
        "        with gr.Column():\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                stop = gr.Button(\"Stop\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        temperature = gr.Slider(\n",
        "                            label=\"Temperature\",\n",
        "                            value=0.1,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Higher values produce more diverse outputs\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_p = gr.Slider(\n",
        "                            label=\"Top-p (nucleus sampling)\",\n",
        "                            value=1.0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1,\n",
        "                            step=0.01,\n",
        "                            interactive=True,\n",
        "                            info=(\n",
        "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
        "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
        "                            ),\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_k = gr.Slider(\n",
        "                            label=\"Top-k\",\n",
        "                            value=50,\n",
        "                            minimum=0.0,\n",
        "                            maximum=200,\n",
        "                            step=1,\n",
        "                            interactive=True,\n",
        "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        repetition_penalty = gr.Slider(\n",
        "                            label=\"Repetition Penalty\",\n",
        "                            value=1.1,\n",
        "                            minimum=1.0,\n",
        "                            maximum=2.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
        "                        )\n",
        "    gr.Examples(examples, inputs=msg, label=\"Click on any example and press the 'Submit' button\")\n",
        "\n",
        "    submit_event = msg.submit(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    submit_click_event = submit.click(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    stop.click(\n",
        "        fn=request_cancel,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        cancels=[submit_event, submit_click_event],\n",
        "        queue=False,\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJkQFgNxxQjY",
        "outputId": "b2c4b85d-d23e-4e08-9079-25ff9541130a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AJ9H4Nj5dN97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load original model from Hugging Face\n",
        "original_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
        "original_model = AutoModelForCausalLM.from_pretrained(original_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2q9QUUMXdU9z"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, history, temperature=0.1, top_p=1.0, top_k=50, repetition_penalty=1.1):\n",
        "    input_ids = convert_history_to_token(history)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "    )\n",
        "    output_ids = model.generate(**generate_kwargs)\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLIOds7JdyTB",
        "outputId": "886491b2-239b-496a-f8a2-7a4672e81cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model:\n",
            "Input: What's your favorite way to spend a weekend?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What's your favorite way to spend a weekend?  \n",
            "<|assistant|>\n",
            "My favorite way to spend a weekend is with my family and friends. I love spending time together, trying new things, and enjoying each other's company. Some of my favorite weekend activities include going for a hike, watching a movie, playing board games, cooking a meal together, and having a picnic in the park. However, depending on the season and the location, there may be different activities that I enjoy more.\n",
            "\n",
            "Input: What inspired you to pursue your current career?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What inspired you to pursue your current career?  \n",
            "<|assistant|>\n",
            "I do not have a personal life or emotions. However, I can provide some insights based on research and data.\n",
            "\n",
            "inspiration for pursuing a career can come from various sources such as personal experiences, family members, friends, mentors, teachers, and other professionals. For some individuals, it may be driven by a desire to make a difference in the world, to learn new skills, or to achieve personal goals. Others may have a passion for a particular field or industry, which sparks their interest and motivates them to pursue a career in that area.\n",
            "\n",
            "some factors that may influence someone's decision to pursue a career include financial stability, job security, career growth opportunities, personal fulfillment, and social mobility. Additionally, individuals may choose a career path based on their values, interests, and personal preferences.\n",
            "\n",
            "overall, pursuing a career requires a combination of personal motivation, intellectual curiosity, and professional development. It takes time, effort, and dedication to build a successful career, but the rewards can be significant.\n",
            "\n",
            "Input: How does machine learning differ from traditional programming?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "How does machine learning differ from traditional programming?  \n",
            "<|assistant|>\n",
            "Machine learning (ML) and traditional programming are two different approaches to software development. Here's how they differ:\n",
            "\n",
            "1. Data Science: ML is a subset of data science that focuses on using algorithms and statistical techniques to analyze and predict data. It involves collecting, cleaning, and preparing data, building models, and testing them to make predictions. Traditional programming, on the other hand, involves writing code to perform specific tasks.\n",
            "\n",
            "2. Algorithms: In ML, algorithms are used to solve problems by finding patterns and relationships in large datasets. These algorithms can be implemented using various programming languages such as Python, R, Java, and C++. Traditional programming, on the other hand, uses procedural programming languages like C++, Java, and Python to write programs.\n",
            "\n",
            "3. Modeling: In ML, models are created to represent the relationship between variables and their values. They are trained using labeled data and then used to make predictions. In traditional programming, models are created using algorithms and tested to make predictions.\n",
            "\n",
            "4. Feedback: In ML, feedback is provided to the model to improve its performance over time. This feedback comes in the form of metrics such as accuracy, precision, recall, and\n",
            "\n",
            "Input: What is the main conflict in the series Arcane?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What is the main conflict in the series Arcane?  \n",
            "<|assistant|>\n",
            "The main conflict in the series Arcane is between the protagonists, Jinx and Valeera, who are both members of the League of Legends organization, and their rival team, the Shadowfiend Academy. The two teams have been feuding for years, with each side trying to gain an advantage over the other through various means, including recruitment, training, and match-ups. As the story progresses, it becomes clear that the root cause of this conflict is Jinx's desire to become the best player in the world and Valeera's determination to protect her teammates from harm. However, as the story unfolds, it becomes apparent that there are deeper issues at play, such as jealousy, betrayal, and personal growth, which ultimately lead to a climactic showdown between the two teams.\n",
            "\n",
            "Input: What are some best practices for debugging code?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some best practices for debugging code?  \n",
            "<|assistant|>\n",
            "Here are some best practices for debugging code:\n",
            "\n",
            "1. Start with a clear understanding of what the code does. This will help you identify where the problem lies.\n",
            "\n",
            "2. Use a debugger (e.g., Visual Studio Code, Eclipse, etc.) to step through the code line by line. This allows you to see exactly what is happening at each point in time.\n",
            "\n",
            "3. Check for errors using tools like the \"console\" or \"trace\" statements. These can help you identify issues with the code, such as missing dependencies, incorrect data types, or invalid syntax.\n",
            "\n",
            "4. Use breakpoints to pause execution at specific points in the code. This allows you to inspect the variables and values at those points, which can help you understand what is going on.\n",
            "\n",
            "5. Test the code thoroughly before deploying it to production. This includes testing for edge cases, ensuring that all dependencies are installed and working correctly, and verifying that the code works as expected.\n",
            "\n",
            "6. Document your code and test results. This helps others who may need to use or modify the code in the future.\n",
            "\n",
            "7. Be patient and persistent. Debugging code can be frustrating and time-consuming, but it\n",
            "\n",
            "Input: Write a poem about the changing seasons.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Write a poem about the changing seasons.  \n",
            "<|assistant|>\n",
            "In autumn's golden hues,\n",
            "The leaves fall like a tapestry,\n",
            "A symphony of colors,\n",
            "As the world prepares for winter's grip.\n",
            "\n",
            "In winter's chill,\n",
            "The snowflakes dance and twirl,\n",
            "A winter wonderland,\n",
            "Where snow-covered trees stand tall.\n",
            "\n",
            "In spring's blooming blossoms,\n",
            "The earth awakens from its slumber,\n",
            "A new life begins anew,\n",
            "As flowers bloom with renewed vigor.\n",
            "\n",
            "In summer's bountiful harvest,\n",
            "The sun beats down on fields and meadows,\n",
            "Fruits ripen, and crops thrive,\n",
            "As the world celebrates its rebirth.\n",
            "\n",
            "In autumn's golden hues,\n",
            "The leaves fall like a tapestry,\n",
            "A symphony of colors,\n",
            "As the world prepares for winter's grip.\n",
            "\n",
            "So let us cherish these changing seasons,\n",
            "And embrace their beauty and grace,\n",
            "For they are a testament to our lives,\n",
            "And remind us of the endless possibilities.\n",
            "\n",
            "Input: What are some effective strategies for expanding your vocabulary?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some effective strategies for expanding your vocabulary?  \n",
            "<|assistant|>\n",
            "Here are some effective strategies for expanding your vocabulary:\n",
            "\n",
            "1. Read widely: Reading books, articles, and newspapers in different languages can help you learn new words and phrases.\n",
            "\n",
            "2. Watch movies and TV shows: Watching movies and TV shows in different languages can expose you to new vocabulary and cultural references.\n",
            "\n",
            "3. Listen to music: Listening to music in different languages can help you learn new words and phrases through their lyrics.\n",
            "\n",
            "4. Take language courses: Taking language courses online or in-person can help you improve your listening, speaking, reading, and writing skills.\n",
            "\n",
            "5. Use language learning apps: There are many language learning apps available that offer interactive exercises, quizzes, and games to help you learn new words and phrases.\n",
            "\n",
            "6. Practice with native speakers: Spending time practicing with native speakers can help you understand their accents, pronunciation, and grammar better.\n",
            "\n",
            "7. Join language exchange programs: Joining language exchange programs with people from different countries can help you practice speaking and understanding new languages.\n",
            "\n",
            "8. Learn from others: Ask friends,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Openvino model:\n",
            "Input: What's your favorite way to spend a weekend?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What's your favorite way to spend a weekend?  \n",
            "<|assistant|>\n",
            "I do not have a personal preference for spending time on weekends. However, here are some common ways people enjoy a weekend:\n",
            "\n",
            "1. Going out with friends and having fun: this could involve going to a concert, watching a movie, trying new restaurants, or just hanging out at home.\n",
            "\n",
            "2. Reading a book or enjoying a good book club: reading can be a relaxing and enjoyable activity.\n",
            "\n",
            "3. Exploring new places: taking a road trip or exploring a new city or town can be an exciting and adventurous way to spend a weekend.\n",
            "\n",
            "4. Watching movies or playing games: watching movies or playing board games or card games can also be a fun way to spend a weekend.\n",
            "\n",
            "5. Spending time with family and loved ones: spending time with family and loved ones can be a special bonding experience.\n",
            "\n",
            "6. Cleaning up the house or doing household chores: cleaning up the house or doing household chores can be a great way to unwind and feel accomplished.\n",
            "\n",
            "7. Relaxing and unwinding: taking a bath, reading a book, or simply sitting by the pool\n",
            "\n",
            "Input: What inspired you to pursue your current career?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What inspired you to pursue your current career?  \n",
            "<|assistant|>\n",
            "I do not have a personal life or emotions like humans do. However, I can provide some general insights into what may have inspired someone to pursue their current career path.\n",
            "\n",
            "in many cases, individuals may have been motivated by a desire to achieve a specific goal or fulfill a dream. For example, someone who wanted to become a doctor may have been inspired by their love for science and medicine, or someone who wants to work in finance may have been drawn to the financial aspect of the industry.\n",
            "\n",
            "another common reason for pursuing a career may be a desire to make a difference in society or contribute to a cause they believe in. This could involve working in non-profit organizations, social justice advocacy, or public service.\n",
            "\n",
            "financial motivations such as earning more money or achieving greater status through a successful career may also play a role.\n",
            "\n",
            "overall, it's difficult to say exactly what inspired someone to pursue a particular career without knowing their personal circumstances and goals. However, there are likely many factors that contributed to their decision, including personal interests, values, and aspirations.\n",
            "\n",
            "Input: How does machine learning differ from traditional programming?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "How does machine learning differ from traditional programming?  \n",
            "<|assistant|>\n",
            "Machine learning (ML) and traditional programming are two different approaches to computer science. Here's how they differ:\n",
            "\n",
            "1. ML is an approach to data analysis and modeling that uses algorithms and statistical techniques to learn from large datasets. It is based on the idea that patterns can be identified and used to predict future outcomes.\n",
            "\n",
            "2. Traditional programming is a set of techniques for creating software applications. It involves writing code to perform specific tasks, such as calculating sums, sorting lists, or displaying graphics.\n",
            "\n",
            "In general, ML is more focused on understanding complex data sets and making predictions, whereas traditional programming is more focused on building software applications. However, both approaches have their own strengths and weaknesses. For example, traditional programming has been around for decades and has a long history of developing software applications, while ML is still relatively new and has only recently gained popularity. Additionally, both approaches require specialized skills and knowledge to use effectively.\n",
            "\n",
            "Overall, while there may be some overlap between the concepts of ML and traditional programming, they are distinct and complementary fields with their own unique challenges and benefits.\n",
            "\n",
            "Input: What is the main conflict in the series Arcane?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What is the main conflict in the series Arcane?  \n",
            "<|assistant|>\n",
            "The main conflict in the series \"Arcane\" is between two factions: the Illuminati and the Shadowhunters. The Illuminati is a secret society dedicated to protecting humanity from demons and other supernatural threats, while the Shadowhunters are a group of humans who have been trained to hunt down and destroy demons. The conflict arises when the two groups clash over control of the demon-hunting organization, which has become a major source of power for both sides. The Illuminati seeks to maintain their secrecy and neutrality, while the Shadowhunters want to use their powers to bring about change and take control of the organization. This conflict ultimately leads to the destruction of the Illuminati and the rise of the Shadowhunters as the new rulers of the demon-hunting world.\n",
            "\n",
            "Input: What are some best practices for debugging code?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some best practices for debugging code?  \n",
            "<|assistant|>\n",
            "Here are some best practices for debugging code:\n",
            "\n",
            "1. Start with a clear understanding of what the code does.\n",
            "2. Use a debugger (e.g., Visual Studio Code, Chrome DevTools) to step through the code line by line.\n",
            "3. Check for errors and exceptions using try-catch blocks.\n",
            "4. Use print statements to log messages during debugging.\n",
            "5. Use breakpoints to pause execution at specific lines.\n",
            "6. Use variable explorers to view variables and their values.\n",
            "7. Use profiling tools to analyze performance issues.\n",
            "8. Use unit tests to verify functionality.\n",
            "9. Use automated testing frameworks like Jest or Mocha to write automated tests.\n",
            "10. Use code coverage tools to measure code coverage.\n",
            "11. Use static analysis tools like SonarQube or Linters to identify coding issues.\n",
            "12. Use code reviews to improve code quality and collaboration.\n",
            "13. Use code refactoring tools like Git Merge or Rebase to simplify and maintain code.\n",
            "14. Use version control systems like Git to track changes and collaborate on code.\n",
            "15. Use code libraries and frameworks like React or Angular to build complex applications\n",
            "\n",
            "Input: Write a poem about the changing seasons.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Write a poem about the changing seasons.  \n",
            "<|assistant|>\n",
            "In springtime, the world awakens anew,\n",
            "A symphony of colors bursting forth,\n",
            "The trees come alive with buds and blossoms,\n",
            "And birds sing their sweet melodies.\n",
            "\n",
            "Summer brings warmth and joy,\n",
            "Fresh air, sunshine, and laughter,\n",
            "The days grow longer, the nights cool,\n",
            "As we embrace this season's grace.\n",
            "\n",
            "Autumn brings crispness and change,\n",
            "Leaves falling from trees, rustling sounds,\n",
            "The scents of pumpkin spice and apple cider,\n",
            "Fill our nostrils, and hearts with delight.\n",
            "\n",
            "Winter brings cold and snowy gloom,\n",
            "Ice-cold winds, snowflakes falling fast,\n",
            "The sound of sleigh bells ringing loud,\n",
            "As we wrap ourselves in blankets and coziness.\n",
            "\n",
            "Each season brings its own unique charm,\n",
            "From the vibrant hues of fall to the chill of winter,\n",
            "We celebrate each one with gratitude,\n",
            "For the beauty that surrounds us all.\n",
            "\n",
            "So let us cherish these changing seasons,\n",
            "And embrace them with\n",
            "\n",
            "Input: What are some effective strategies for expanding your vocabulary?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some effective strategies for expanding your vocabulary?  \n",
            "<|assistant|>\n",
            "1. Read widely: Reading books, magazines, newspapers, and blogs can help you expand your vocabulary by exposing you to new words and phrases.\n",
            "\n",
            "2. Watch movies and TV shows: Watching movies and TV shows with subtitles can help you learn new words and phrases.\n",
            "\n",
            "3. Take language courses: Attending language classes or taking online language courses can help you improve your listening, speaking, reading, and writing skills.\n",
            "\n",
            "4. Practice with language apps: There are many language apps available on both iOS and Android devices that can help you practice speaking and understanding new words and phrases.\n",
            "\n",
            "5. Join language exchange groups: Joining language exchange groups on social media platforms like Facebook or Meetup can help you practice speaking with native speakers from around the world.\n",
            "\n",
            "6. Learn through podcasts: Listening to podcasts about different topics can help you learn new vocabulary and phrases related to those topics.\n",
            "\n",
            "7. Use language learning apps: There are many language learning apps available on both iOS and Android devices that can help you learn new vocabulary and phrases.\n",
            "\n",
            "8. Study with\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Original model average inference time: 101.85 seconds\n",
            "Openvino model average inference time: 63.11 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Iterate over examples\n",
        "print(\"Original model:\")\n",
        "total_time = 0\n",
        "for example in examples:\n",
        "    history = [[example[0], \"\"]]  # Initialize history with the current user input\n",
        "    start_time = time.time()\n",
        "    response = generate_response(original_model, history)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "    print(f\"Input: {example[0]}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()\n",
        "\n",
        "original_time = total_time/len(examples)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(f\"Openvino model:\")\n",
        "total_time = 0\n",
        "for example in examples:\n",
        "    history = [[example[0], \"\"]]  # Initialize history with the current user input\n",
        "    start_time = time.time()\n",
        "    response = generate_response(ov_model, history)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "    print(f\"Input: {example[0]}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()\n",
        "\n",
        "openvino_time = total_time/len(examples)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(f\"Original model average inference time: {original_time:.2f} seconds\")\n",
        "print(f\"Openvino model average inference time: {openvino_time:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c81ea07b356b4dc0930d09e0925107d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f6467111f6449229609c3a615c15a5a",
              "IPY_MODEL_2376af59aa2a411c830757f1f7fcee72",
              "IPY_MODEL_91189da9874c48e1a916dadbb499bee3",
              "IPY_MODEL_629c5e879a024e62afe47b3ae3ff0b89"
            ],
            "layout": "IPY_MODEL_287befd8c3ef4a298c8cc06d8d902af2"
          }
        },
        "48a405d762ee4cec97b265412e3194ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe15f4c98065468f970aaf5e4b4a6132",
            "placeholder": "​",
            "style": "IPY_MODEL_5a6a339928e6457c8967f2a6ef214397",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "44a757b7d2d0448eb4d7693ac85033e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_271b3bf299ae456990657ca017910957",
            "placeholder": "​",
            "style": "IPY_MODEL_d9022c5909bd448f8e35bd80fd8fd30a",
            "value": ""
          }
        },
        "f818f346dce34eb293e2e1d02a0620b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_11e38469ff5641ae8286b7b91de5cbe1",
            "style": "IPY_MODEL_0c0f46f668964aab98756bca8dcb0aaf",
            "value": true
          }
        },
        "d41a3078b9144b5cbb794de44a2e0f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2a3952c416934611b64dc6c96299d437",
            "style": "IPY_MODEL_214fb6f006ff4ee0b17a3eaa4ebde83d",
            "tooltip": ""
          }
        },
        "b92ebfa18135471b980c2a13a8e867f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841c385620f34883bbeb7f78881f97d6",
            "placeholder": "​",
            "style": "IPY_MODEL_31168702fea441bb8edca14d69a908a7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "287befd8c3ef4a298c8cc06d8d902af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fe15f4c98065468f970aaf5e4b4a6132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a6a339928e6457c8967f2a6ef214397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "271b3bf299ae456990657ca017910957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9022c5909bd448f8e35bd80fd8fd30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11e38469ff5641ae8286b7b91de5cbe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0f46f668964aab98756bca8dcb0aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a3952c416934611b64dc6c96299d437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214fb6f006ff4ee0b17a3eaa4ebde83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "841c385620f34883bbeb7f78881f97d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31168702fea441bb8edca14d69a908a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c9d10b9cf0f421b86fd118570a816e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2bf16ee83fd47ffb4cd536bbdb13efb",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0d50e939904c61a6ae4c1e9a187b19",
            "value": "Connecting..."
          }
        },
        "b2bf16ee83fd47ffb4cd536bbdb13efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0d50e939904c61a6ae4c1e9a187b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f6467111f6449229609c3a615c15a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89f848c0f9d443c49183ab19ef6ba031",
            "placeholder": "​",
            "style": "IPY_MODEL_69536d5f4d674205a00c54c0b6af6051",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "2376af59aa2a411c830757f1f7fcee72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0fba71c22d4a8989829eec0d311f0e",
            "placeholder": "​",
            "style": "IPY_MODEL_03060c54251b4dc28796c678fbd39d10",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "91189da9874c48e1a916dadbb499bee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a313ea293d99499dbebee43bc60d055e",
            "placeholder": "​",
            "style": "IPY_MODEL_18dee6ba9b0840b988dc7ca65b0ae453",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "629c5e879a024e62afe47b3ae3ff0b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4859cb9e1ab7408ea2523f71edc531bf",
            "placeholder": "​",
            "style": "IPY_MODEL_7e3847f3f1714a918cd2b5c244f88c60",
            "value": "Login successful"
          }
        },
        "89f848c0f9d443c49183ab19ef6ba031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69536d5f4d674205a00c54c0b6af6051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0fba71c22d4a8989829eec0d311f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03060c54251b4dc28796c678fbd39d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a313ea293d99499dbebee43bc60d055e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dee6ba9b0840b988dc7ca65b0ae453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4859cb9e1ab7408ea2523f71edc531bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3847f3f1714a918cd2b5c244f88c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}